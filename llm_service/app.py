"""FastAPI application for generating chat completions using a pretrained and optimized Qwen2.5.1 model."""

from typing import Any, Dict, List

from fastapi import FastAPI
from llama_cpp import Llama
from pydantic import BaseModel, Field

app = FastAPI(
    title="Chat LLM API",
    description="A microservice API for generating chat completions using a pretrained Llama model.",
    version="0.1.0",
)


class ChatMessage(BaseModel):
    """Model representing a single chat message."""

    role: str = Field(
        description="The role of the message (e.g., 'system', 'user', or 'assistant').",
        example="user",
    )
    content: str = Field(
        description="The content of the message.", example="What is the capital of France?"
    )


class ChatRequest(BaseModel):
    """Request model for chat completion."""

    messages: List[ChatMessage] = Field(
        description="A list of chat messages for the conversation.",
        example=[
            {"role": "system", "content": "You are a helpful agent."},
            {"role": "user", "content": "What is the capital of France?"},
        ],
    )


class ChatResponse(BaseModel):
    """Response model for chat completion."""

    answer: str = Field(description="The cleaned answer generated by the LLM.", example="Paris")


# Initialize the LLM model
llm: Llama = Llama.from_pretrained(
    repo_id="bartowski/Qwen2.5.1-Coder-7B-Instruct-GGUF",
    filename="Qwen2.5.1-Coder-7B-Instruct-IQ3_XS.gguf",
)


@app.post(
    "/chat",
    response_model=ChatResponse,
    summary="Generate chat completion",
    description="Generate a chat response from a list of conversation messages using a pretrained Llama model.",
)
async def generate_chat_completion(request: ChatRequest) -> ChatResponse:
    """Generate a chat response using the Llama model.

    This endpoint accepts a list of chat messages, forwards them to the LLM,
    cleans the generated response, and returns the answer.

    Parameters
    ----------
    request : ChatRequest
        The chat request containing conversation messages.

    Returns
    -------
    ChatResponse
        The response containing the cleaned answer from the LLM.
    """
    # Convert ChatMessage models to dictionaries
    messages: List[Dict[str, Any]] = [message.dict() for message in request.messages]
    # Create chat completion using the provided messages
    result: Dict[str, Any] = llm.create_chat_completion(messages=messages)

    # Extract the raw answer from the result
    answer: str = result["choices"][0]["message"]["content"]
    return ChatResponse(answer=answer)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
